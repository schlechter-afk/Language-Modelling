{"cells":[{"cell_type":"code","execution_count":9,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-08T15:14:55.585597Z","iopub.status.busy":"2024-09-08T15:14:55.584951Z","iopub.status.idle":"2024-09-08T15:14:55.592034Z","shell.execute_reply":"2024-09-08T15:14:55.591147Z","shell.execute_reply.started":"2024-09-08T15:14:55.585555Z"},"trusted":true},"outputs":[],"source":["import re\n","import random\n","from sklearn.model_selection import train_test_split\n","import spacy\n","import fasttext.util\n","import gensim\n","import functools\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","from torch.nn.utils.rnn import pad_sequence\n","from gensim.models import FastText\n","import pickle\n","import os\n","import gc\n","import numpy as np\n","from tqdm import tqdm\n","import math"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-09-08T15:14:55.594078Z","iopub.status.busy":"2024-09-08T15:14:55.593796Z","iopub.status.idle":"2024-09-08T15:14:55.606850Z","shell.execute_reply":"2024-09-08T15:14:55.606009Z","shell.execute_reply.started":"2024-09-08T15:14:55.594047Z"},"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","DATA_DIR = '/kaggle/input/anlp-q3-data/'\n","\n","VAL_SPLIT = 0.2\n","TEST_SPLIT = 0.1"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-09-08T15:14:55.608150Z","iopub.status.busy":"2024-09-08T15:14:55.607863Z","iopub.status.idle":"2024-09-08T15:14:55.808810Z","shell.execute_reply":"2024-09-08T15:14:55.807837Z","shell.execute_reply.started":"2024-09-08T15:14:55.608120Z"},"trusted":true},"outputs":[{"data":{"text/plain":["2982"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-09-08T15:14:55.812160Z","iopub.status.busy":"2024-09-08T15:14:55.811408Z","iopub.status.idle":"2024-09-08T15:15:26.686733Z","shell.execute_reply":"2024-09-08T15:15:26.685725Z","shell.execute_reply.started":"2024-09-08T15:14:55.812121Z"},"trusted":true},"outputs":[],"source":["class FastTextEmbeddingGenerator:\n","    def __init__(self):\n","        self.model = None\n","\n","    def set_model(self, model):\n","        self.model = model\n","    \n","    def get_embedding(self, word):\n","        if word in self.model:\n","            embedding = self.model[word]\n","            return embedding\n","        else:\n","            embedding = self.model.get_word_vector(word)\n","            return embedding\n","\n","# # fasttext.util.download_model('en', if_exists='ignore')  # English\n","# kagglehub.model_download('swayam421/embedding-pretrained/Other/default/1')\n","\n","ft = fasttext.load_model('/kaggle/input/emb-gen/other/default/1/cc.en.300.bin')\n","embedding_gen = FastTextEmbeddingGenerator()\n","embedding_gen.set_model(ft)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-09-08T15:15:26.688441Z","iopub.status.busy":"2024-09-08T15:15:26.688104Z","iopub.status.idle":"2024-09-08T15:19:16.585545Z","shell.execute_reply":"2024-09-08T15:19:16.584566Z","shell.execute_reply.started":"2024-09-08T15:15:26.688407Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of sentences: 55298\n"]}],"source":["class LanguageModelDataset:\n","    def __init__(self, file_path, chunk_size=100000):\n","        self.file_path = file_path\n","        self.chunk_size = chunk_size\n","        self.nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\n","        self.sentences = self._process_large_file()\n","        self.train_sentences = None\n","        self.val_sentences = None\n","        self.test_sentences = None\n","        # self.max_sentence_length = max(len(sentence.split()) for sentence in self.sentences)  # Calculate global max sentence length\n","        self.max_sentence_length = 80  # Calculate global max sentence length\n","\n","    # @functools.lru_cache(maxsize=None)\n","    def _process_large_file(self):\n","        sentences = []\n","        c = 0\n","        with open(self.file_path, 'r', encoding='utf-8') as file:\n","            buffer = \"\"\n","            for line in file:\n","\n","#                 if c > 100:\n","#                     break\n","\n","                line = line.strip()\n","                if line: \n","\n","                    if buffer:\n","                        buffer += \" \" + line\n","                    else:\n","                        buffer = line\n","\n","                else: \n","\n","                    if buffer:\n","                        temp_sentences = buffer.split(\".\")\n","                        for sentence in temp_sentences:\n","                            sentence = sentence.strip()\n","                            sentence = re.sub(r\"[^a-zA-Z0-9\\s]+\", '', sentence)\n","                            sentence = sentence.strip()\n","                            # preprocess text\n","                            preprocessed_text = self._preprocess_text(sentence)\n","                            if preprocessed_text:\n","                                c += 1\n","                                sentences.append(preprocessed_text)\n","\n","                        buffer = \"\"\n","\n","            if buffer:\n","                buffer = buffer.strip()\n","                buffer = re.sub(r\"[^a-zA-Z0-9\\s]+\", '', buffer)\n","                temp_sentences = buffer.split(\".\")\n","                for sentence in temp_sentences:\n","                    sentence = sentence.strip()\n","                    preprocessed_text = self._preprocess_text(sentence)\n","                    if preprocessed_text:\n","                        sentences.append(preprocessed_text)\n","                # sentences.append(self._preprocess_text(buffer))\n","\n","        # sentences = [sentence for sentence in sentences if sentence != \"\"]\n","        return sentences\n","\n","    # @functools.lru_cache(maxsize=None)\n","    def _preprocess_text(self, text):\n","        doc = self.nlp(text)\n","        sentences = \" \".join([token.text for token in doc])\n","        return sentences\n","\n","    # @functools.lru_cache(maxsize=None)\n","    def get_splits(self, val_size=10000, test_size=20000):\n","        train_sentences, val_test_sentences = train_test_split(self.sentences, test_size=val_size+test_size, shuffle=False, random_state=42)\n","        test_size = test_size / (val_size + test_size)\n","        val_sentences, test_sentences = train_test_split(val_test_sentences, test_size=test_size, shuffle=False, random_state=42)\n","        self.train_sentences = train_sentences\n","        self.val_sentences = val_sentences\n","        self.test_sentences = test_sentences\n","        return train_sentences, val_sentences, test_sentences\n","    \n","    def build_vocab(self):\n","        vocab = set()\n","        for sentence in self.train_sentences:\n","            for word in sentence.split():\n","                vocab.add(word)\n","        self.vocab = list(vocab)\n","        self.word2idx = {word: idx for idx, word in enumerate(self.vocab)}\n","        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n","\n","file_path = DATA_DIR + 'Auguste_Maquet.txt'\n","dataset = LanguageModelDataset(file_path)\n","print(f\"Total number of sentences: {len(dataset.sentences)}\")\n","train_sentences, val_sentences, test_sentences = dataset.get_splits(val_size=VAL_SPLIT, test_size=TEST_SPLIT)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-09-08T15:19:16.587199Z","iopub.status.busy":"2024-09-08T15:19:16.586822Z","iopub.status.idle":"2024-09-08T15:19:16.742573Z","shell.execute_reply":"2024-09-08T15:19:16.741383Z","shell.execute_reply.started":"2024-09-08T15:19:16.587155Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Length of train_sentences: 38708\n","Length of val_sentences: 11060\n","Length of test_sentences: 5530\n","Number of training sentences: 38708\n"]}],"source":["print(f\"Length of train_sentences: {len(train_sentences)}\")\n","print(f\"Length of val_sentences: {len(val_sentences)}\")\n","print(f\"Length of test_sentences: {len(test_sentences)}\")\n","\n","dataset.build_vocab() # Build vocabulary\n","\n","word2idx = dataset.word2idx\n","idx2word = dataset.idx2word\n","\n","# add UNK token\n","word2idx['<UNK>'] = len(word2idx)\n","idx2word[len(idx2word)] = '<UNK>'\n","dataset.vocab.append('<UNK>')\n","\n","# add PAD token\n","word2idx['<PAD>'] = len(word2idx)\n","idx2word[len(idx2word)] = '<PAD>'\n","dataset.vocab.append('<PAD>')\n","\n","vocab_size = len(word2idx)\n","\n","print(f\"Number of training sentences: {len(train_sentences)}\")"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-09-08T15:19:16.744047Z","iopub.status.busy":"2024-09-08T15:19:16.743764Z","iopub.status.idle":"2024-09-08T15:19:16.751523Z","shell.execute_reply":"2024-09-08T15:19:16.750422Z","shell.execute_reply.started":"2024-09-08T15:19:16.744017Z"},"trusted":true},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.encoding = torch.zeros(max_len, d_model)\n","\n","        position = torch.arange(0, max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * -(torch.log(torch.tensor(10000.0)) / d_model))\n","\n","        self.encoding[:, 0::2] = torch.sin(position * div_term)\n","        self.encoding[:, 1::2] = torch.cos(position * div_term)\n","        # self.encoding = self.encoding.unsqueeze(0)  # Add a batch dimension\n","\n","    def forward(self, x):\n","        seq_len = x.size(1)\n","        return self.encoding[:seq_len, :]"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-09-08T15:19:16.752991Z","iopub.status.busy":"2024-09-08T15:19:16.752704Z","iopub.status.idle":"2024-09-08T15:19:16.866280Z","shell.execute_reply":"2024-09-08T15:19:16.865304Z","shell.execute_reply.started":"2024-09-08T15:19:16.752961Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["80\n"]}],"source":["class SequentialData(Dataset):\n","    def __init__(self, sentences, embedding_gen, word2idx, max_len, d_model):\n","        self.sentences = sentences\n","        self.embedding_gen = embedding_gen\n","        self.word2idx = word2idx\n","        self.max_len = max_len\n","        self.pad_token = '<PAD>'\n","        self.pad_idx = word2idx[self.pad_token]\n","        self.positional_encoding = PositionalEncoding(d_model, max_len)\n","        self.d_model = d_model\n","\n","    def __len__(self):\n","        return len(self.sentences)\n","\n","    def __getitem__(self, idx):\n","        try:\n","            sentence = self.sentences[idx]\n","            sentence = sentence.lower()\n","            sentence = sentence[:self.max_len]  # Truncate sentence to max_len\n","            tokens = sentence.split()\n","            sentence_embedding = [self.embedding_gen.get_embedding(token) if token in self.word2idx else self.embedding_gen.get_embedding('<UNK>') for token in tokens]\n","            target_indices = [self.word2idx[token] if token in self.word2idx else self.word2idx['<UNK>'] for token in tokens]\n","\n","            padding_len = self.max_len - len(tokens)\n","            if padding_len > 0:\n","                sentence_embedding.extend([np.zeros_like(sentence_embedding[0])] * padding_len)  # Pad embeddings with zero vectors\n","                target_indices.extend([self.pad_idx] * padding_len)  # Pad indices with pad_idx\n","            \n","            # convert sentence_embedding to tensor\n","            sentence_embedding = np.array(sentence_embedding)\n","\n","            # print(f\"Sentence Embedding Shape Before Anything new: {sentence_embedding.shape}\")\n","            padding_mask = [0 if i >= len(tokens) else 1 for i in range(self.max_len)]\n","\n","            # Padding mask is 0 for padded tokens and 1 for tokens that are part of the sequence\n","            # -- in alignment with the transformer paper\n","\n","            sentence_embedding = torch.tensor(sentence_embedding).float()\n","            target_indices = torch.tensor(target_indices)\n","            \n","            # print(f\"Sentence Embedding Shape: {sentence_embedding.shape}\")\n","            # print(f\"Target Indices Shape: {target_indices.shape}\")\n","            # return sentence_embedding, target_indices, torch.tensor(padding_mask)\n","        \n","            positional_encoding = self.positional_encoding(sentence_embedding)  # Get positional encodings\n","            sentence_embedding = sentence_embedding + positional_encoding  # Fuse FastText and Positional Embeddings\n","\n","            return np.array(sentence_embedding), np.array(target_indices), torch.tensor(padding_mask)\n","        \n","        except Exception as e:\n","            print(f\"Error processing sequence: {self.sentences[idx]}\")\n","            print(f\"Error as {e}\")\n","            return None\n","\n","d_model = 300\n","train_dataset = SequentialData(train_sentences, embedding_gen, word2idx, dataset.max_sentence_length, d_model)\n","train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=True)\n","print(dataset.max_sentence_length)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-09-08T15:19:16.867792Z","iopub.status.busy":"2024-09-08T15:19:16.867431Z","iopub.status.idle":"2024-09-08T15:19:16.880190Z","shell.execute_reply":"2024-09-08T15:19:16.879037Z","shell.execute_reply.started":"2024-09-08T15:19:16.867750Z"},"trusted":true},"outputs":[],"source":["class TransformerBlock(nn.Module):\n","    def __init__(self, hidden_size=128, num_heads=4):\n","        super(TransformerBlock, self).__init__()\n","        self.norm1 = nn.LayerNorm(hidden_size)\n","        self.multihead_attn = nn.MultiheadAttention(hidden_size, num_heads=num_heads, batch_first=True, dropout=0.1)\n","        self.norm2 = nn.LayerNorm(hidden_size)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(hidden_size, hidden_size * 4),\n","            nn.ELU(),\n","            nn.Linear(hidden_size * 4, hidden_size)\n","        )\n","\n","    def forward(self, x, padding_mask, attn_mask):\n","        norm_x = self.norm1(x)\n","        attn_output, _ = self.multihead_attn(norm_x, norm_x, norm_x, attn_mask=attn_mask, key_padding_mask=padding_mask)\n","        x = attn_output + x\n","        norm_x = self.norm2(x)\n","        x = self.mlp(norm_x) + x\n","        return x\n","\n","class Transformer(nn.Module):\n","    def __init__(self, num_emb, hidden_size=128, num_layers=3, num_heads=4):\n","        super(Transformer, self).__init__()\n","        self.blocks = nn.ModuleList([TransformerBlock(hidden_size, num_heads) for _ in range(num_layers)])\n","        self.fc_out = nn.Linear(hidden_size, num_emb)\n","        self.vocab_size = num_emb\n","\n","    def generate_square_subsequent_mask(self, sz):\n","        # Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n","        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n","        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","        return mask\n","\n","    def forward(self, x, tgt_mask, padding_mask):\n","        # x already contains the embeddings and positional encodings\n","        for block in self.blocks:\n","            x = block(x, padding_mask, attn_mask=tgt_mask)\n","        return self.fc_out(x)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-09-08T15:19:16.883621Z","iopub.status.busy":"2024-09-08T15:19:16.883271Z","iopub.status.idle":"2024-09-08T15:19:16.896177Z","shell.execute_reply":"2024-09-08T15:19:16.895388Z","shell.execute_reply.started":"2024-09-08T15:19:16.883585Z"},"trusted":true},"outputs":[],"source":["def train(model, train_loader, optimizer, criterion, device, clip_grad=1.0):\n","    model.train()\n","    total_loss = 0\n","\n","    progress_bar = tqdm(train_loader, desc=\"Training\")\n","    for batch_idx, (data, target, padding_mask) in enumerate(progress_bar):\n","        gc.collect()\n","        torch.cuda.empty_cache()   \n","        \n","        data, target, padding_mask = data.to(device), target.to(device), padding_mask.to(device) \n","        optimizer.zero_grad()\n","        \n","        input_seq = data[:, :-1, :]\n","        target_seq = target[:, 1:]\n","\n","        # print(f\"Input Sequence Shape: {input_seq.shape}\")\n","\n","        # padding mask is 0 for padded tokens and 1 for tokens that are part of the sequence\n","        tgt_key_padding_mask = (padding_mask[:, :-1] == 0)\n","        \n","        tgt_mask = model.generate_square_subsequent_mask(input_seq.size(1)).to(device)\n","\n","        output = model(input_seq, tgt_mask=tgt_mask, padding_mask=tgt_key_padding_mask)\n","        output_reshape = output.view(-1, model.vocab_size)\n","        target_seq_reshape = target_seq.reshape(-1)\n","\n","        loss = criterion(output_reshape, target_seq_reshape) \n","\n","        del data, target, padding_mask, input_seq, target_seq, tgt_key_padding_mask, tgt_mask, output, output_reshape, target_seq_reshape\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","        loss.backward()\n","\n","        # torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n","\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","        progress_bar.set_postfix({\"Running Training Loss\": total_loss / (batch_idx + 1)})\n","    \n","    return total_loss / len(train_loader)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-09-08T15:19:16.897942Z","iopub.status.busy":"2024-09-08T15:19:16.897604Z","iopub.status.idle":"2024-09-08T15:19:16.911047Z","shell.execute_reply":"2024-09-08T15:19:16.910165Z","shell.execute_reply.started":"2024-09-08T15:19:16.897902Z"},"trusted":true},"outputs":[],"source":["# Evaluate on test set\n","val_dataset = SequentialData(val_sentences, embedding_gen, word2idx, dataset.max_sentence_length, d_model)\n","val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n","\n","def validate(model, test_loader, criterion, device):\n","    model.eval()\n","    total_loss = 0\n","\n","    progress_bar = tqdm(test_loader, desc=\"Validation\")\n","    with torch.no_grad():\n","        for batch_idx, (data, target, padding_mask) in enumerate(progress_bar):\n","            data, target, padding_mask = data.to(device), target.to(device), padding_mask.to(device)\n","\n","            input_seq = data[:, :-1, :]\n","            target_seq = target[:, 1:]\n","\n","            tgt_key_padding_mask = (padding_mask[:, :-1] == 0)\n","\n","            tgt_mask = model.generate_square_subsequent_mask(input_seq.size(1)).to(device)\n","\n","            output = model(input_seq, tgt_mask=tgt_mask, padding_mask=tgt_key_padding_mask)\n","\n","            output_reshape = output.view(-1, model.vocab_size)\n","            target_seq_reshape = target_seq.reshape(-1)\n","            loss = criterion(output_reshape, target_seq_reshape)\n","\n","            total_loss += loss.item()\n","            progress_bar.set_postfix({\"Running Validation Loss\": total_loss / (batch_idx + 1)})\n","    \n","    return total_loss / len(test_loader)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-09-08T15:19:16.912454Z","iopub.status.busy":"2024-09-08T15:19:16.912136Z","iopub.status.idle":"2024-09-08T17:31:18.986347Z","shell.execute_reply":"2024-09-08T17:31:18.985377Z","shell.execute_reply.started":"2024-09-08T15:19:16.912422Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Training:   0%|          | 0/302 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:5193: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n","Training: 100%|██████████| 302/302 [21:03<00:00,  4.19s/it, Running Training Loss=5.84]\n","Validation: 100%|██████████| 87/87 [05:10<00:00,  3.56s/it, Running Validation Loss=5.33]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/5, Train Loss: 5.8422, Val Loss: 5.3311\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 302/302 [21:14<00:00,  4.22s/it, Running Training Loss=5.22]\n","Validation: 100%|██████████| 87/87 [05:13<00:00,  3.60s/it, Running Validation Loss=5.18]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/5, Train Loss: 5.2176, Val Loss: 5.1756\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 302/302 [21:21<00:00,  4.24s/it, Running Training Loss=5.01]\n","Validation: 100%|██████████| 87/87 [05:14<00:00,  3.61s/it, Running Validation Loss=5.08]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/5, Train Loss: 5.0083, Val Loss: 5.0842\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 302/302 [21:20<00:00,  4.24s/it, Running Training Loss=4.87]\n","Validation: 100%|██████████| 87/87 [05:11<00:00,  3.58s/it, Running Validation Loss=5.09]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/5, Train Loss: 4.8676, Val Loss: 5.0902\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 302/302 [21:00<00:00,  4.17s/it, Running Training Loss=4.75]\n","Validation: 100%|██████████| 87/87 [05:11<00:00,  3.58s/it, Running Validation Loss=5.05]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5/5, Train Loss: 4.7500, Val Loss: 5.0465\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["hidden_size = 300\n","num_layers = 1\n","num_heads = 6\n","model = Transformer(vocab_size, hidden_size, num_layers, num_heads).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","criterion = nn.CrossEntropyLoss(ignore_index=word2idx['<PAD>'])\n","\n","# Training loop\n","num_epochs = 5\n","\n","# del train_dataset, val_dataset\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","for epoch in range(num_epochs):\n","    loss = train(model, train_loader, optimizer, criterion, device)\n","    val_loss = validate(model, val_loader, criterion, device)\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss:.4f}, Val Loss: {val_loss:.4f}\")"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-09-08T17:31:18.988163Z","iopub.status.busy":"2024-09-08T17:31:18.987606Z","iopub.status.idle":"2024-09-08T17:31:19.043797Z","shell.execute_reply":"2024-09-08T17:31:19.042871Z","shell.execute_reply.started":"2024-09-08T17:31:18.988128Z"},"trusted":true},"outputs":[],"source":["torch.save(model, 'transformer_q3.pth')"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-09-08T17:31:19.045798Z","iopub.status.busy":"2024-09-08T17:31:19.045205Z","iopub.status.idle":"2024-09-08T17:33:51.619768Z","shell.execute_reply":"2024-09-08T17:33:51.618805Z","shell.execute_reply.started":"2024-09-08T17:31:19.045760Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Validation: 100%|██████████| 44/44 [02:32<00:00,  3.47s/it, Running Validation Loss=5.05]"]},{"name":"stdout","output_type":"stream","text":["Test Loss: 5.0506\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Evaluate on test set\n","test_dataset = SequentialData(test_sentences, embedding_gen, word2idx, dataset.max_sentence_length, d_model)\n","test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n","\n","test_loss = validate(model, test_loader, criterion, device)\n","print(f\"Test Loss: {test_loss:.4f}\")"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-09-08T17:33:51.621903Z","iopub.status.busy":"2024-09-08T17:33:51.621520Z","iopub.status.idle":"2024-09-08T17:56:42.424800Z","shell.execute_reply":"2024-09-08T17:56:42.423719Z","shell.execute_reply.started":"2024-09-08T17:33:51.621858Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["In Progress Evaluation: 100%|██████████| 38708/38708 [19:56<00:00, 32.34it/s, Running Validation Loss=nan]\n","In Progress Evaluation: 100%|██████████| 5530/5530 [02:52<00:00, 31.98it/s, Running Validation Loss=nan]\n"]},{"name":"stdout","output_type":"stream","text":["All Done!\n"]}],"source":["def evaluate_sentence_wise(model, test_loader, criterion, device):\n","    model.eval()\n","    total_loss = 0\n","    perplexities = []\n","    progress_bar = tqdm(test_loader, desc=\"In Progress Evaluation\")\n","    with torch.no_grad():\n","        for batch_idx, (data, target, padding_mask) in enumerate(progress_bar):\n","            data, target, padding_mask = data.to(device), target.to(device), padding_mask.to(device)\n","\n","            input_seq = data[:, :-1, :]\n","            target_seq = target[:, 1:]\n","\n","            tgt_key_padding_mask = (padding_mask[:, :-1] == 0)\n","\n","            tgt_mask = model.generate_square_subsequent_mask(input_seq.size(1)).to(device)\n","\n","            output = model(input_seq, tgt_mask=tgt_mask, padding_mask=tgt_key_padding_mask)\n","\n","            output_reshape = output.view(-1, model.vocab_size)\n","            target_seq_reshape = target_seq.reshape(-1)\n","            loss = criterion(output_reshape, target_seq_reshape)\n","            curr_perplexity = torch.exp(loss)\n","            perplexities.append(curr_perplexity)\n","            total_loss += loss.item()\n","            progress_bar.set_postfix({\"Running Validation Loss\": total_loss / (batch_idx + 1)})\n","    \n","    # convert perplexities to numpy array by first loading them to cpu\n","    perplexities = [perplexity.cpu().numpy() for perplexity in perplexities]\n","    return np.array(perplexities)\n","\n","eval_train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n","eval_test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n","\n","train_perplexities = evaluate_sentence_wise(model, eval_train_loader, criterion, device)\n","\n","test_perplexities = evaluate_sentence_wise(model, eval_test_loader, criterion, device)\n","\n","print(\"All Done!\")"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-09-08T18:22:10.217114Z","iopub.status.busy":"2024-09-08T18:22:10.216686Z","iopub.status.idle":"2024-09-08T18:22:10.437284Z","shell.execute_reply":"2024-09-08T18:22:10.436407Z","shell.execute_reply.started":"2024-09-08T18:22:10.217073Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Average Test Perplexity (Trimmed): 137.5290069580078\n","Average Train Perplexity (Trimmed): 102.2426986694336\n","Median Test Perplexity: 105.2917938232422\n","Median Train Perplexity: 87.03754425048828\n","All Done!\n"]}],"source":["# create a pair of sentence, perplexity for train and test set\n","train_sentence_perplexity = [(sentence, perplexity) for sentence, perplexity in zip(train_sentences, train_perplexities)]\n","test_sentence_perplexity = [(sentence, perplexity) for sentence, perplexity in zip(test_sentences, test_perplexities)]\n","\n","# exclude all pairs where the train sentence is less than 2 tokens\n","train_sentence_perplexity = [pair for pair in train_sentence_perplexity if len(pair[0].split()) > 1]\n","test_sentence_perplexity = [pair for pair in test_sentence_perplexity if len(pair[0].split()) > 1]\n","\n","train_perplexities_filtered = [pair[1] for pair in train_sentence_perplexity]\n","test_perplexities_filtered = [pair[1] for pair in test_sentence_perplexity]\n","\n","avg_train_perplexity = np.mean(train_perplexities_filtered)\n","avg_test_perplexity = np.mean(test_perplexities_filtered)\n","\n","from scipy.stats import trim_mean\n","trimmed_train_perplexity = trim_mean(train_perplexities_filtered, 0.1)\n","trimmed_test_perplexity = trim_mean(test_perplexities_filtered, 0.1)\n","\n","median_train_perplexity = np.median(train_perplexities_filtered)\n","median_test_perplexity = np.median(test_perplexities_filtered)\n","\n","print(f\"Average Test Perplexity (Trimmed): {trimmed_test_perplexity}\")\n","print(f\"Average Train Perplexity (Trimmed): {trimmed_train_perplexity}\")\n","\n","print(f\"Median Test Perplexity: {median_test_perplexity}\")\n","print(f\"Median Train Perplexity: {median_train_perplexity}\")\n","\n","# write the pairs to a file\n","with open('2021101068-LM3-train-perplexity.txt', 'w') as file:\n","    for sentence, perplexity in train_sentence_perplexity:\n","        file.write(f\"{sentence}\\t{perplexity.item()}\\n\")\n","    \n","    file.write('\\n')\n","    file.write(f\"Average Perplexity: {avg_train_perplexity}\\n\")\n","    file.write(f\"Average Perplexity (Trimmed -- excluding 0.1% from both ends): {trimmed_train_perplexity}\\n\")\n","    file.write(f\"Median Perplexity: {median_train_perplexity}\\n\")\n","\n","with open('2021101068-LM3-test-perplexity.txt', 'w') as file:\n","    for sentence, perplexity in test_sentence_perplexity:\n","        file.write(f\"{sentence}\\t{perplexity.item()}\\n\")\n","\n","    file.write('\\n')\n","    file.write(f\"Average Perplexity: {avg_test_perplexity}\\n\")\n","    file.write(f\"Average Perplexity (Trimmed -- excluding 0.1% from both ends): {trimmed_test_perplexity}\\n\")\n","    file.write(f\"Median Perplexity: {median_test_perplexity}\\n\")\n","\n","print(\"All Done!\")"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-09-08T18:22:46.918923Z","iopub.status.busy":"2024-09-08T18:22:46.918455Z","iopub.status.idle":"2024-09-08T18:22:48.093598Z","shell.execute_reply":"2024-09-08T18:22:48.092481Z","shell.execute_reply.started":"2024-09-08T18:22:46.918874Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["perplexities_q3_test.txt   test_sentence_perplexity_q3.txt\n","perplexities_q3_train.txt  train_sentence_perplexity_q3.txt\n","state.db\t\t   transformer_q3.pth\n"]},{"data":{"text/html":["<a href='test_sentence_perplexity_q3.txt' target='_blank'>test_sentence_perplexity_q3.txt</a><br>"],"text/plain":["/kaggle/working/test_sentence_perplexity_q3.txt"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<a href='train_sentence_perplexity_q3.txt' target='_blank'>train_sentence_perplexity_q3.txt</a><br>"],"text/plain":["/kaggle/working/train_sentence_perplexity_q3.txt"]},"metadata":{},"output_type":"display_data"}],"source":["!ls /kaggle/working\n","\n","from IPython.display import FileLink\n","\n","display(FileLink('test_sentence_perplexity_q3.txt'))\n","\n","display(FileLink('train_sentence_perplexity_q3.txt'))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5663016,"sourceId":9343952,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelId":116204,"modelInstanceId":91999,"sourceId":109846,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30761,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
