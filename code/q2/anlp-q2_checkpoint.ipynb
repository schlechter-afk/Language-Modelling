{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "import fasttext.util\n",
    "import gensim\n",
    "import functools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from gensim.models import FastText\n",
    "import pickle\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DATA_DIR = '../data/'\n",
    "\n",
    "VAL_SPLIT = 0.2\n",
    "TEST_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastTextEmbeddingGenerator:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def get_embedding(self, word):\n",
    "        if word in self.model:\n",
    "            embedding = self.model[word]\n",
    "            return embedding\n",
    "        else:\n",
    "            embedding = self.model.get_word_vector(word)\n",
    "            return embedding\n",
    "\n",
    "# fasttext.util.download_model('en', if_exists='ignore')        \n",
    "# os.remove('cc.en.300.bin.gz')\n",
    "ft = fasttext.load_model('cc.en.300.bin')\n",
    "embedding_gen = FastTextEmbeddingGenerator()\n",
    "embedding_gen.set_model(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelDataset:\n",
    "    def __init__(self, file_path, chunk_size=100000):\n",
    "        self.file_path = file_path\n",
    "        self.chunk_size = chunk_size\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\n",
    "        self.sentences = self._process_large_file()\n",
    "        self.train_sentences = None\n",
    "        self.val_sentences = None\n",
    "        self.test_sentences = None\n",
    "        # self.max_sentence_length = max(len(sentence.split()) for sentence in self.sentences)  # Calculate global max sentence length\n",
    "        self.max_sentence_length = 80  # Calculate global max sentence length\n",
    "\n",
    "    # @functools.lru_cache(maxsize=None)\n",
    "    def _process_large_file(self):\n",
    "        sentences = []\n",
    "        c = 0\n",
    "        with open(self.file_path, 'r', encoding='utf-8') as file:\n",
    "            buffer = \"\"\n",
    "            for line in file:\n",
    "\n",
    "                # if c > 97:\n",
    "                #     break\n",
    "\n",
    "                line = line.strip()\n",
    "                if line: \n",
    "\n",
    "                    if buffer:\n",
    "                        buffer += \" \" + line\n",
    "                    else:\n",
    "                        buffer = line\n",
    "\n",
    "                else: \n",
    "\n",
    "                    if buffer:\n",
    "                        temp_sentences = buffer.split(\".\")\n",
    "                        for sentence in temp_sentences:\n",
    "                            sentence = sentence.strip()\n",
    "                            sentence = re.sub(r\"[^a-zA-Z0-9\\s]+\", '', sentence)\n",
    "                            sentence = sentence.strip()\n",
    "                            # preprocess text\n",
    "                            preprocessed_text = self._preprocess_text(sentence)\n",
    "                            if preprocessed_text:\n",
    "                                c += 1\n",
    "                                sentences.append(preprocessed_text)\n",
    "\n",
    "                        buffer = \"\"\n",
    "\n",
    "            if buffer:\n",
    "                buffer = buffer.strip()\n",
    "                buffer = re.sub(r\"[^a-zA-Z0-9\\s]+\", '', buffer)\n",
    "                temp_sentences = buffer.split(\".\")\n",
    "                for sentence in temp_sentences:\n",
    "                    sentence = sentence.strip()\n",
    "                    preprocessed_text = self._preprocess_text(sentence)\n",
    "                    if preprocessed_text:\n",
    "                        sentences.append(preprocessed_text)\n",
    "                # sentences.append(self._preprocess_text(buffer))\n",
    "\n",
    "        # sentences = [sentence for sentence in sentences if sentence != \"\"]\n",
    "        return sentences\n",
    "\n",
    "    # @functools.lru_cache(maxsize=None)\n",
    "    def _preprocess_text(self, text):\n",
    "        doc = self.nlp(text)\n",
    "        sentences = \" \".join([token.text for token in doc])\n",
    "        return sentences\n",
    "\n",
    "    # @functools.lru_cache(maxsize=None)\n",
    "    def get_splits(self, val_size=10000, test_size=20000):\n",
    "        train_sentences, val_test_sentences = train_test_split(self.sentences, test_size=val_size+test_size, shuffle=False, random_state=42)\n",
    "        test_size = test_size / (val_size + test_size)\n",
    "        val_sentences, test_sentences = train_test_split(val_test_sentences, test_size=test_size, shuffle=False, random_state=42)\n",
    "        self.train_sentences = train_sentences\n",
    "        self.val_sentences = val_sentences\n",
    "        self.test_sentences = test_sentences\n",
    "        return train_sentences, val_sentences, test_sentences\n",
    "    \n",
    "    def build_vocab(self):\n",
    "        vocab = set()\n",
    "        for sentence in self.train_sentences:\n",
    "            for word in sentence.split():\n",
    "                vocab.add(word)\n",
    "        self.vocab = list(vocab)\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences: 55298\n"
     ]
    }
   ],
   "source": [
    "file_path = DATA_DIR + 'Auguste_Maquet.txt'\n",
    "dataset = LanguageModelDataset(file_path)\n",
    "print(f\"Total number of sentences: {len(dataset.sentences)}\")\n",
    "train_sentences, val_sentences, test_sentences = dataset.get_splits(val_size=VAL_SPLIT, test_size=TEST_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the test sentences to a file\n",
    "with open(DATA_DIR + 'test_sentences.txt', 'w') as f:\n",
    "    for sentence in test_sentences:\n",
    "        f.write(sentence + '\\n')\n",
    "\n",
    "with open(DATA_DIR + 'val_sentences.txt', 'w') as f:\n",
    "    for sentence in val_sentences:\n",
    "        f.write(sentence + '\\n')\n",
    "\n",
    "with open(DATA_DIR + 'train_sentences.txt', 'w') as f:\n",
    "    for sentence in train_sentences:\n",
    "        f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the corpus to a text file\n",
    "with open(DATA_DIR + 'processed_corpus_modified.txt', 'w') as f:\n",
    "    for sentence in dataset.sentences:\n",
    "        f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This website includes information about Project Gutenberg including how to make donations to the Project Gutenberg Literary Archive Foundation how to help produce our new eBooks and how to subscribe to our email newsletter to hear about new eBooks\n"
     ]
    }
   ],
   "source": [
    "print(dataset.sentences[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_sentences: 38708\n",
      "Length of val_sentences: 11060\n",
      "Length of test_sentences: 5530\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of train_sentences: {len(train_sentences)}\")\n",
    "print(f\"Length of val_sentences: {len(val_sentences)}\")\n",
    "print(f\"Length of test_sentences: {len(test_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 38708\n"
     ]
    }
   ],
   "source": [
    "dataset.build_vocab() # Build vocabulary\n",
    "\n",
    "word2idx = dataset.word2idx\n",
    "idx2word = dataset.idx2word\n",
    "\n",
    "# add UNK token\n",
    "word2idx['<UNK>'] = len(word2idx)\n",
    "idx2word[len(idx2word)] = '<UNK>'\n",
    "dataset.vocab.append('<UNK>')\n",
    "\n",
    "# add PAD token\n",
    "word2idx['<PAD>'] = len(word2idx)\n",
    "idx2word[len(idx2word)] = '<PAD>'\n",
    "dataset.vocab.append('<PAD>')\n",
    "\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "print(f\"Number of training sentences: {len(train_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialData(Dataset):\n",
    "    def __init__(self, sentences, embedding_gen, word2idx, max_len):\n",
    "        self.sentences = sentences\n",
    "        self.embedding_gen = embedding_gen\n",
    "        self.word2idx = word2idx\n",
    "        self.max_len = max_len\n",
    "        self.pad_token = '<PAD>'\n",
    "        self.pad_idx = word2idx[self.pad_token]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            sentence = self.sentences[idx]\n",
    "            # convert to lowercase\n",
    "            sentence = sentence.lower()\n",
    "            sentence = sentence[:self.max_len]  # Truncate sentence to max_len\n",
    "            tokens = sentence.split()\n",
    "\n",
    "            sentence_embedding = [self.embedding_gen.get_embedding(token) if token in self.word2idx else self.embedding_gen.get_embedding('<UNK>') for token in tokens]\n",
    "            target_indices = [self.word2idx[token] if token in self.word2idx else self.word2idx['<UNK>'] for token in tokens]\n",
    "\n",
    "            # Padding\n",
    "            padding_len = self.max_len - len(tokens)\n",
    "            if padding_len > 0:\n",
    "                sentence_embedding.extend([np.zeros_like(sentence_embedding[0])] * padding_len)  # Pad embeddings with zero vectors\n",
    "                target_indices.extend([self.pad_idx] * padding_len)  # Pad indices with pad_idx\n",
    "            \n",
    "            return np.array(sentence_embedding), np.array(target_indices)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sequence: {self.sentences[idx]}\")\n",
    "            print(f\"Error: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader created.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SequentialData(train_sentences, embedding_gen, word2idx, dataset.max_sentence_length)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "print(\"DataLoader created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        logits = self.fc(lstm_out)\n",
    "        return logits, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(1, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                weight.new(1, batch_size, self.hidden_dim).zero_().to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 300\n",
    "hidden_dim = 300\n",
    "model = LSTMModel(embedding_dim, hidden_dim, vocab_size)\n",
    "\n",
    "model.to(device) # Move model to GPU if available\n",
    " \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2idx['<PAD>'])\n",
    "\n",
    "n_epochs = 5\n",
    "\n",
    "torch.cuda.empty_cache() # Clear cache before training\n",
    "\n",
    "print(\"Starting training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Number of batches: 1209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   4%|▎         | 45/1209 [00:47<17:52,  1.09it/s, Running Loss: 7.5315]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1209/1209 [16:44<00:00,  1.20it/s, Running Loss: 5.8759]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 5.8759\n",
      "Epoch 2\n",
      "Number of batches: 1209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1209/1209 [16:39<00:00,  1.21it/s, Running Loss: 5.1525]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 5.1525\n",
      "Epoch 3\n",
      "Number of batches: 1209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1209/1209 [16:35<00:00,  1.21it/s, Running Loss: 4.8504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 4.8504\n",
      "Epoch 4\n",
      "Number of batches: 1209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1209/1209 [16:37<00:00,  1.21it/s, Running Loss: 4.6383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 4.6383\n",
      "Epoch 5\n",
      "Number of batches: 1209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1209/1209 [16:38<00:00,  1.21it/s, Running Loss: 4.4584]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 4.4584\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    print(f\"Number of batches: {len(train_loader)}\")\n",
    "\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    \n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "\n",
    "    pbar.set_description(f\"Epoch {epoch+1}\")\n",
    "\n",
    "    for i, batch in pbar:\n",
    "\n",
    "        sentence_embeddings, sentence_indices = batch\n",
    "\n",
    "        sentence_embeddings, sentence_indices = sentence_embeddings.to(device), sentence_indices.to(device)\n",
    "        hidden = tuple(h.detach() for h in hidden)\n",
    "        inputs = sentence_embeddings[:, :-1, :] # Cut off last token\n",
    "        targets = sentence_indices[:, 1:]  # Shift targets by one position\n",
    "        del sentence_embeddings\n",
    "\n",
    "        output, hidden = model(inputs, hidden)\n",
    "        del inputs\n",
    "\n",
    "        output = output.view(-1, vocab_size)  # Shape: [batch_size * seq_len, vocab_size]\n",
    "        targets = targets.reshape(-1)  # Shape: [batch_size * seq_len]\n",
    "\n",
    "        loss = criterion(output, targets)\n",
    "        del sentence_indices, targets, output\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        pbar.set_postfix_str(f\"Running Loss: {total_loss / (i+1):.4f}\")\n",
    "\n",
    "        del loss\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final average loss:  4.45837258247426\n",
      "Final perplexity:  86.34687227672048\n",
      "Model saved at ../data/lm_q2_modified_fix_2.pth\n"
     ]
    }
   ],
   "source": [
    "print(\"Final average loss: \", avg_loss)\n",
    "print(\"Final perplexity: \", np.exp(avg_loss))\n",
    "\n",
    "model_complete_path = os.path.join(DATA_DIR, 'lm_q2_modified_fix_2.pth')\n",
    "print(f\"Model saved at {model_complete_path}\")\n",
    "\n",
    "torch.save(model, model_complete_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_complete_path = os.path.join(DATA_DIR, 'lm_q2_modified_fix_2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on validation set\n",
    "def calculate_perplexity(loss):  # Calculate perplexity from loss value: [Source](https://hackernoon.com/crossentropy-logloss-and-perplexity-different-facets-of-likelihood)\n",
    "    return np.exp(loss)\n",
    "\n",
    "def evaluate_model(model, data_loader, vocab_size, criterion):\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_perplexities = []\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in data_loader:\n",
    "\n",
    "            sentence_embeddings, sentence_indices = batch\n",
    "            sentence_embeddings, sentence_indices = sentence_embeddings.to(device), sentence_indices.to(device)\n",
    "\n",
    "            hidden = tuple(h.detach() for h in hidden)\n",
    "            inputs = sentence_embeddings[:, :-1, :]  # Cut off last token\n",
    "            targets = sentence_indices[:, 1:]  # Shift targets by one position\n",
    "            output, hidden = model(inputs, hidden)\n",
    "            # print(f\"Output shape before reshape: {output.shape}\")  \n",
    "            output = output.view(-1, vocab_size)  # Shape: [batch_size * seq_len, vocab_size]\n",
    "            targets = targets.reshape(-1)  # Shape: [batch_size * seq_len]\n",
    "\n",
    "            # print(f\"Output  shape: {output.shape}\")\n",
    "            # print(f\"Targets shape: {targets.shape}\")\n",
    "            \n",
    "            loss = criterion(output, targets)\n",
    "\n",
    "            print(\"Loss value for this batch: \", loss.item())\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            perplexity = calculate_perplexity(loss.item())\n",
    "            \n",
    "            all_perplexities.append(perplexity)\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    avg_perplexity = np.mean(all_perplexities)\n",
    "    \n",
    "    return all_perplexities, avg_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5530\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "print(len(test_sentences))\n",
    "\n",
    "save_test_sentences = copy.deepcopy(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5530\n"
     ]
    }
   ],
   "source": [
    "print(len(save_test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on GPU\n",
      "Loss value for this batch:  5.22805643081665\n",
      "Loss value for this batch:  4.564521312713623\n",
      "Loss value for this batch:  4.523038864135742\n",
      "Loss value for this batch:  4.5558977127075195\n",
      "Loss value for this batch:  4.8082661628723145\n",
      "Loss value for this batch:  4.974948883056641\n",
      "Loss value for this batch:  4.648752689361572\n",
      "Loss value for this batch:  4.81417989730835\n",
      "Loss value for this batch:  4.656454563140869\n",
      "Loss value for this batch:  4.611695766448975\n",
      "Loss value for this batch:  5.241815090179443\n",
      "Loss value for this batch:  5.055790901184082\n",
      "Loss value for this batch:  5.037288665771484\n",
      "Loss value for this batch:  4.987472057342529\n",
      "Loss value for this batch:  4.841423988342285\n",
      "Loss value for this batch:  4.900551795959473\n",
      "Loss value for this batch:  4.978673934936523\n",
      "Loss value for this batch:  5.090958595275879\n",
      "Loss value for this batch:  5.173855781555176\n",
      "Loss value for this batch:  4.937090873718262\n",
      "Loss value for this batch:  4.591442108154297\n",
      "Loss value for this batch:  4.790920257568359\n",
      "Loss value for this batch:  4.615163803100586\n",
      "Loss value for this batch:  5.522100925445557\n",
      "Loss value for this batch:  4.746208190917969\n",
      "Loss value for this batch:  4.737162113189697\n",
      "Loss value for this batch:  4.669841289520264\n",
      "Loss value for this batch:  4.600226402282715\n",
      "Loss value for this batch:  4.559097766876221\n",
      "Loss value for this batch:  4.774207592010498\n",
      "Loss value for this batch:  4.453114032745361\n",
      "Loss value for this batch:  4.325616359710693\n",
      "Loss value for this batch:  4.690546035766602\n",
      "Loss value for this batch:  5.03948974609375\n",
      "Loss value for this batch:  4.626980781555176\n",
      "Loss value for this batch:  4.716650009155273\n",
      "Loss value for this batch:  5.154321670532227\n",
      "Loss value for this batch:  4.5499444007873535\n",
      "Loss value for this batch:  4.818306922912598\n",
      "Loss value for this batch:  4.809218406677246\n",
      "Loss value for this batch:  4.847842216491699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss value for this batch:  4.629284381866455\n",
      "Loss value for this batch:  5.067140579223633\n",
      "Loss value for this batch:  4.592826843261719\n",
      "Loss value for this batch:  5.014926433563232\n",
      "Loss value for this batch:  4.689857482910156\n",
      "Loss value for this batch:  4.635814189910889\n",
      "Loss value for this batch:  4.124967098236084\n",
      "Loss value for this batch:  5.253499984741211\n",
      "Loss value for this batch:  4.976517677307129\n",
      "Loss value for this batch:  5.097686767578125\n",
      "Loss value for this batch:  4.78342866897583\n",
      "Loss value for this batch:  5.202695846557617\n",
      "Loss value for this batch:  5.072206974029541\n",
      "Loss value for this batch:  5.136517524719238\n",
      "Loss value for this batch:  4.960639476776123\n",
      "Loss value for this batch:  4.785879135131836\n",
      "Loss value for this batch:  4.691222190856934\n",
      "Loss value for this batch:  4.634783744812012\n",
      "Loss value for this batch:  4.962592601776123\n",
      "Loss value for this batch:  4.597593307495117\n",
      "Loss value for this batch:  5.032094478607178\n",
      "Loss value for this batch:  5.199658393859863\n",
      "Loss value for this batch:  4.807046413421631\n",
      "Loss value for this batch:  4.9258317947387695\n",
      "Loss value for this batch:  5.069097995758057\n",
      "Loss value for this batch:  4.600005149841309\n",
      "Loss value for this batch:  4.827402114868164\n",
      "Loss value for this batch:  5.111871719360352\n",
      "Loss value for this batch:  4.991095066070557\n",
      "Loss value for this batch:  5.3930230140686035\n",
      "Loss value for this batch:  5.415316581726074\n",
      "Loss value for this batch:  4.988032817840576\n",
      "Loss value for this batch:  4.662424564361572\n",
      "Loss value for this batch:  4.875702381134033\n",
      "Loss value for this batch:  5.117292881011963\n",
      "Loss value for this batch:  5.333839416503906\n",
      "Loss value for this batch:  4.705869197845459\n",
      "Loss value for this batch:  4.787998199462891\n",
      "Loss value for this batch:  5.110233783721924\n",
      "Loss value for this batch:  4.987524032592773\n",
      "Loss value for this batch:  4.925942897796631\n",
      "Loss value for this batch:  5.395387649536133\n",
      "Loss value for this batch:  5.0497660636901855\n",
      "Loss value for this batch:  4.789071083068848\n",
      "Loss value for this batch:  4.909964561462402\n",
      "Loss value for this batch:  4.91418981552124\n",
      "Loss value for this batch:  4.621098518371582\n",
      "Loss value for this batch:  4.935309410095215\n",
      "Loss value for this batch:  5.300731182098389\n",
      "Loss value for this batch:  4.869363307952881\n",
      "Loss value for this batch:  4.679958343505859\n",
      "Loss value for this batch:  4.447222709655762\n",
      "Loss value for this batch:  5.560203552246094\n",
      "Loss value for this batch:  4.831745147705078\n",
      "Loss value for this batch:  4.81551456451416\n",
      "Loss value for this batch:  5.418662071228027\n",
      "Loss value for this batch:  5.177042007446289\n",
      "Loss value for this batch:  5.342624664306641\n",
      "Loss value for this batch:  4.8772172927856445\n",
      "Loss value for this batch:  4.963957786560059\n",
      "Loss value for this batch:  4.937066078186035\n",
      "Loss value for this batch:  5.012979030609131\n",
      "Loss value for this batch:  5.523496627807617\n",
      "Loss value for this batch:  4.893671989440918\n",
      "Loss value for this batch:  5.028388977050781\n",
      "Loss value for this batch:  4.999593734741211\n",
      "Loss value for this batch:  4.998471260070801\n",
      "Loss value for this batch:  4.957992076873779\n",
      "Loss value for this batch:  4.578039646148682\n",
      "Loss value for this batch:  4.728353500366211\n",
      "Loss value for this batch:  4.994030475616455\n",
      "Loss value for this batch:  4.643241882324219\n",
      "Loss value for this batch:  4.757824420928955\n",
      "Loss value for this batch:  5.018145561218262\n",
      "Loss value for this batch:  4.383400917053223\n",
      "Loss value for this batch:  4.673954963684082\n",
      "Loss value for this batch:  4.551449775695801\n",
      "Loss value for this batch:  4.991593360900879\n",
      "Loss value for this batch:  4.844310760498047\n",
      "Loss value for this batch:  5.037015438079834\n",
      "Loss value for this batch:  4.558610439300537\n",
      "Loss value for this batch:  5.061677932739258\n",
      "Loss value for this batch:  5.23598051071167\n",
      "Loss value for this batch:  4.855449676513672\n",
      "Loss value for this batch:  4.687789440155029\n",
      "Loss value for this batch:  5.238134860992432\n",
      "Loss value for this batch:  4.630917549133301\n",
      "Loss value for this batch:  4.818384647369385\n",
      "Loss value for this batch:  4.623996734619141\n",
      "Loss value for this batch:  4.535809516906738\n",
      "Loss value for this batch:  4.549778461456299\n",
      "Loss value for this batch:  4.4638872146606445\n",
      "Loss value for this batch:  4.618858337402344\n",
      "Loss value for this batch:  4.4128098487854\n",
      "Loss value for this batch:  5.048038959503174\n",
      "Loss value for this batch:  4.537347316741943\n",
      "Loss value for this batch:  4.9798970222473145\n",
      "Loss value for this batch:  4.734058856964111\n",
      "Loss value for this batch:  4.598550796508789\n",
      "Loss value for this batch:  5.079229831695557\n",
      "Loss value for this batch:  4.880376815795898\n",
      "Loss value for this batch:  5.0489959716796875\n",
      "Loss value for this batch:  4.420715808868408\n",
      "Loss value for this batch:  4.730448246002197\n",
      "Loss value for this batch:  4.791208267211914\n",
      "Loss value for this batch:  4.767180919647217\n",
      "Loss value for this batch:  4.688874244689941\n",
      "Loss value for this batch:  4.972714900970459\n",
      "Loss value for this batch:  4.771629810333252\n",
      "Loss value for this batch:  4.791097640991211\n",
      "Loss value for this batch:  4.69242000579834\n",
      "Loss value for this batch:  4.7665324211120605\n",
      "Loss value for this batch:  4.764026641845703\n",
      "Loss value for this batch:  4.98211145401001\n",
      "Loss value for this batch:  5.071378231048584\n",
      "Loss value for this batch:  5.228747367858887\n",
      "Loss value for this batch:  4.74269962310791\n",
      "Loss value for this batch:  5.106898307800293\n",
      "Loss value for this batch:  4.679667949676514\n",
      "Loss value for this batch:  4.422715663909912\n",
      "Loss value for this batch:  4.676275253295898\n",
      "Loss value for this batch:  5.029727935791016\n",
      "Loss value for this batch:  4.704490661621094\n",
      "Loss value for this batch:  4.722315788269043\n",
      "Loss value for this batch:  5.119100570678711\n",
      "Loss value for this batch:  4.165788650512695\n",
      "Loss value for this batch:  4.171652793884277\n",
      "Loss value for this batch:  4.557356357574463\n",
      "Loss value for this batch:  4.6367011070251465\n",
      "Loss value for this batch:  4.684584140777588\n",
      "Loss value for this batch:  4.940552234649658\n",
      "Length of all perplexities: 172\n",
      "Averge of all perplexities:  132.6125575389193\n",
      "Average perplexity on test set: 132.6126\n"
     ]
    }
   ],
   "source": [
    "test_dataset = SequentialData(test_sentences, embedding_gen, word2idx, dataset.max_sentence_length)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "# model.load_state_dict(torch.load(model_path))\n",
    "# Load the entire model from model_complete_path\n",
    "# model = torch.load(model_complete_path)\n",
    "# check on which device model is\n",
    "if next(model.parameters()).is_cuda:\n",
    "    print('Model is on GPU')\n",
    "else:\n",
    "    print('Model is on CPU')\n",
    "    model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_perplexities, avg_perplexity = evaluate_model(model, test_loader, vocab_size, criterion)\n",
    "print(f\"Length of all perplexities: {len(all_perplexities)}\")\n",
    "print(\"Averge of all perplexities: \", np.mean(all_perplexities))\n",
    "print(f\"Average perplexity on test set: {avg_perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import trim_mean\n",
    "\n",
    "class SequentialDataWithSentence(Dataset):\n",
    "    def __init__(self, sentences, embedding_gen, word2idx, max_len):\n",
    "        self.sentences = sentences\n",
    "        self.embedding_gen = embedding_gen\n",
    "        self.word2idx = word2idx\n",
    "        self.max_len = max_len\n",
    "        self.pad_token = '<PAD>'\n",
    "        self.pad_idx = word2idx[self.pad_token]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            sentence = self.sentences[idx]\n",
    "            # convert to lowercase\n",
    "            sentence = sentence.lower()\n",
    "            sentence = sentence[:self.max_len]  # Truncate sentence to max_len\n",
    "            tokens = sentence.split()\n",
    "\n",
    "            sentence_embedding = [self.embedding_gen.get_embedding(token) if token in self.word2idx else self.embedding_gen.get_embedding('<UNK>') for token in tokens]\n",
    "            target_indices = [self.word2idx[token] if token in self.word2idx else self.word2idx['<UNK>'] for token in tokens]\n",
    "\n",
    "            # Padding\n",
    "            padding_len = self.max_len - len(tokens)\n",
    "            if padding_len > 0:\n",
    "                sentence_embedding.extend([np.zeros_like(sentence_embedding[0])] * padding_len)  # Pad embeddings with zero vectors\n",
    "                target_indices.extend([self.pad_idx] * padding_len)  # Pad indices with pad_idx\n",
    "            \n",
    "            return np.array(sentence_embedding), np.array(target_indices), sentence\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sequence: {self.sentences[idx]}\")\n",
    "            print(f\"Error: {e}\")\n",
    "            return None\n",
    "        \n",
    "test_dataset = SequentialDataWithSentence(save_test_sentences, embedding_gen, word2idx, dataset.max_sentence_length)\n",
    "\n",
    "eval_batch_size = 1\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=eval_batch_size, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def calculate_perplexity_per_sentence(model, data_loader, vocab_size, criterion):\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_perplexities = []\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in data_loader:\n",
    "\n",
    "            sentence_embeddings, sentence_indices, sentence = batch\n",
    "            sentence_embeddings, sentence_indices = sentence_embeddings.to(device), sentence_indices.to(device)\n",
    "\n",
    "            hidden = tuple(h.detach() for h in hidden)\n",
    "            inputs = sentence_embeddings[:, :-1, :]  # Cut off last token\n",
    "            targets = sentence_indices[:, 1:]  # Shift targets by one position\n",
    "            output, hidden = model(inputs, hidden)\n",
    "            output = output.view(-1, vocab_size)  # Shape: [batch_size * seq_len, vocab_size]\n",
    "            targets = targets.reshape(-1)  # Shape: [batch_size * seq_len]\n",
    "            \n",
    "            loss = criterion(output, targets)\n",
    "\n",
    "            print(\"Loss value for this batch: \", loss.item())\n",
    "\n",
    "            perplexity = calculate_perplexity(loss.item())\n",
    "            \n",
    "            all_perplexities.append((perplexity, sentence))\n",
    "\n",
    "    return all_perplexities\n",
    "\n",
    "all_perplexities = calculate_perplexity_per_sentence(model, test_loader, vocab_size, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_perplexities = [pair for pair in all_perplexities if len(pair[1][0].split()) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perplexities = [perplexity for perplexity, sentence in all_perplexities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average perplexity on test set: 16002.6611\n",
      "Trimmed mean perplexity on test set: 159.0638\n",
      "Median perplexity on test set: 119.8522\n"
     ]
    }
   ],
   "source": [
    "# exclude the entries with nan perplexity\n",
    "test_perplexities = np.array(test_perplexities)\n",
    "avg_perplexity = np.mean(test_perplexities)\n",
    "avg_trimmed_perplexity = trim_mean(test_perplexities, 0.1)\n",
    "median_perplexity = np.median(test_perplexities)\n",
    "\n",
    "print(f\"Average perplexity on test set: {avg_perplexity:.4f}\")\n",
    "print(f\"Trimmed mean perplexity on test set: {avg_trimmed_perplexity:.4f}\")\n",
    "print(f\"Median perplexity on test set: {median_perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexities written to file.\n"
     ]
    }
   ],
   "source": [
    "# Write the perplexities to a file\n",
    "with open('2021101068-LM2-test-perplexity.txt', 'w') as f:\n",
    "    for perplexity, sentence in all_perplexities:\n",
    "        f.write(f\"{sentence[0]}\\t{perplexity}\\n\")\n",
    "\n",
    "    f.write('\\n')\n",
    "    f.write(f\"Average Perplexity: {avg_perplexity}\\n\")\n",
    "    f.write(f\"Average Perplexity (Trimmed -- excluding 0.1% from both ends): {avg_trimmed_perplexity}\\n\")\n",
    "    f.write(f\"Median Perplexity: {median_perplexity}\\n\")\n",
    "\n",
    "print(\"Perplexities written to file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the perplexities to a file for train set \n",
    "train_dataset = SequentialDataWithSentence(train_sentences, embedding_gen, word2idx, dataset.max_sentence_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=eval_batch_size, shuffle=False)\n",
    "\n",
    "all_perplexities = calculate_perplexity_per_sentence(model, train_loader, vocab_size, criterion)\n",
    "print(f\"Length of all perplexities: {len(all_perplexities)}\")\n",
    "all_perplexities = [(perplexity, sentence) for perplexity, sentence in all_perplexities if len(sentence[0].split()) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexities written to file.\n"
     ]
    }
   ],
   "source": [
    "train_perplexities = [perplexity for perplexity, sentence in all_perplexities]\n",
    "\n",
    "with open('2021101068-LM2-train-perplexity.txt', 'w') as f:\n",
    "    for perplexity, sentence in all_perplexities:\n",
    "        f.write(f\"{sentence[0]}\\t{perplexity}\\n\")\n",
    "\n",
    "    f.write('\\n')\n",
    "    f.write(f\"Average Perplexity: {np.mean(train_perplexities)}\\n\")\n",
    "    f.write(f\"Average Perplexity (Trimmed -- excluding 0.1% from both ends): {trim_mean(train_perplexities, proportiontocut=0.1)}\\n\")\n",
    "    f.write(f\"Median Perplexity: {np.median(train_perplexities)}\\n\")\n",
    "\n",
    "print(\"Perplexities written to file.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_lip_loc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
