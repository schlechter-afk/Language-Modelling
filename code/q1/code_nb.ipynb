{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "import fasttext.util\n",
    "import gensim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from gensim.models import FastText\n",
    "import pickle\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DATA_DIR = '../../data/'\n",
    "\n",
    "VAL_SPLIT = 0.2\n",
    "TEST_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastTextEmbeddingGenerator:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def get_embedding(self, word):\n",
    "        if word in self.model:\n",
    "            embedding = self.model[word]\n",
    "            return embedding\n",
    "        else:\n",
    "            embedding = self.model.get_word_vector(word)\n",
    "            return embedding\n",
    "        \n",
    "# download the model first\n",
    "fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "ft = fasttext.load_model('../cc.en.300.bin')\n",
    "embedding_gen = FastTextEmbeddingGenerator()\n",
    "embedding_gen.set_model(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelDataset:\n",
    "    def __init__(self, file_path, chunk_size=100000):\n",
    "        self.file_path = file_path\n",
    "        self.chunk_size = chunk_size\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\n",
    "        self.sentences = self._process_large_file()\n",
    "        self.train_sentences = None\n",
    "        self.val_sentences = None\n",
    "        self.test_sentences = None\n",
    "        # self.max_sentence_length = max(len(sentence.split()) for sentence in self.sentences)  # Calculate global max sentence length\n",
    "        self.max_sentence_length = 80  # Calculate global max sentence length\n",
    "\n",
    "    # @functools.lru_cache(maxsize=None)\n",
    "    def _process_large_file(self):\n",
    "        sentences = []\n",
    "        c = 0\n",
    "        with open(self.file_path, 'r', encoding='utf-8') as file:\n",
    "            buffer = \"\"\n",
    "            for line in file:\n",
    "\n",
    "                if c > 100:\n",
    "                    break\n",
    "\n",
    "                line = line.strip()\n",
    "                if line: \n",
    "\n",
    "                    if buffer:\n",
    "                        buffer += \" \" + line\n",
    "                    else:\n",
    "                        buffer = line\n",
    "\n",
    "                else: \n",
    "\n",
    "                    if buffer:\n",
    "                        temp_sentences = buffer.split(\".\")\n",
    "                        for sentence in temp_sentences:\n",
    "                            sentence = sentence.strip()\n",
    "                            sentence = re.sub(r\"[^a-zA-Z0-9\\s]+\", '', sentence)\n",
    "                            sentence = sentence.strip()\n",
    "                            # preprocess text\n",
    "                            preprocessed_text = self._preprocess_text(sentence)\n",
    "                            if preprocessed_text:\n",
    "                                c += 1\n",
    "                                sentences.append(preprocessed_text)\n",
    "\n",
    "                        buffer = \"\"\n",
    "\n",
    "            if buffer:\n",
    "                buffer = buffer.strip()\n",
    "                buffer = re.sub(r\"[^a-zA-Z0-9\\s]+\", '', buffer)\n",
    "                temp_sentences = buffer.split(\".\")\n",
    "                for sentence in temp_sentences:\n",
    "                    sentence = sentence.strip()\n",
    "                    preprocessed_text = self._preprocess_text(sentence)\n",
    "                    if preprocessed_text:\n",
    "                        sentences.append(preprocessed_text)\n",
    "                # sentences.append(self._preprocess_text(buffer))\n",
    "\n",
    "        # sentences = [sentence for sentence in sentences if sentence != \"\"]\n",
    "        return sentences\n",
    "\n",
    "    # @functools.lru_cache(maxsize=None)\n",
    "    def _preprocess_text(self, text):\n",
    "        doc = self.nlp(text)\n",
    "        sentences = \" \".join([token.text for token in doc])\n",
    "        return sentences\n",
    "\n",
    "    # @functools.lru_cache(maxsize=None)\n",
    "    def get_splits(self, val_size=10000, test_size=20000):\n",
    "        train_sentences, val_test_sentences = train_test_split(self.sentences, test_size=val_size+test_size, shuffle=False, random_state=42)\n",
    "        test_size = test_size / (val_size + test_size)\n",
    "        val_sentences, test_sentences = train_test_split(val_test_sentences, test_size=test_size, shuffle=False, random_state=42)\n",
    "        self.train_sentences = train_sentences\n",
    "        self.val_sentences = val_sentences\n",
    "        self.test_sentences = test_sentences\n",
    "        return train_sentences, val_sentences, test_sentences\n",
    "    \n",
    "    def build_vocab(self):\n",
    "        vocab = set()\n",
    "        for sentence in self.train_sentences:\n",
    "            for word in sentence.split():\n",
    "                vocab.add(word)\n",
    "        self.vocab = list(vocab)\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences: 199\n"
     ]
    }
   ],
   "source": [
    "file_path = DATA_DIR + 'Auguste_Maquet.txt'\n",
    "dataset = LanguageModelDataset(file_path)\n",
    "print(f\"Total number of sentences: {len(dataset.sentences)}\")\n",
    "train_sentences, val_sentences, test_sentences = dataset.get_splits(val_size=VAL_SPLIT, test_size=TEST_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_sentences: 139\n",
      "Length of val_sentences: 40\n",
      "Length of test_sentences: 20\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of train_sentences: {len(train_sentences)}\")\n",
    "print(f\"Length of val_sentences: {len(val_sentences)}\")\n",
    "print(f\"Length of test_sentences: {len(test_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 139\n"
     ]
    }
   ],
   "source": [
    "dataset.build_vocab() # Build vocabulary\n",
    "\n",
    "word2idx = dataset.word2idx\n",
    "idx2word = dataset.idx2word\n",
    "\n",
    "# add UNK token\n",
    "word2idx['<UNK>'] = len(word2idx)\n",
    "idx2word[len(idx2word)] = '<UNK>'\n",
    "dataset.vocab.append('<UNK>')\n",
    "\n",
    "# add PAD token\n",
    "word2idx['<PAD>'] = len(word2idx)\n",
    "idx2word[len(idx2word)] = '<PAD>'\n",
    "dataset.vocab.append('<PAD>')\n",
    "\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "print(f\"Number of training sentences: {len(train_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def extract_data(sentences, word2idx, embedding_gen, n_gram=5):\n",
    "    inputs, targets = [], []\n",
    "\n",
    "    for sentence in tqdm(sentences, desc=\"Processing Sentences\"):\n",
    "        try:\n",
    "            words = sentence.split()\n",
    "\n",
    "            # Precompute embeddings for all words in the sentence\n",
    "            embeddings = np.array([embedding_gen.get_embedding(word) if word in word2idx else embedding_gen.get_embedding('<UNK>') for word in words], dtype=np.float32)\n",
    "            embeddings = torch.tensor(embeddings, dtype=torch.float32).to(device)\n",
    "            # Create sliding windows of 5-gram context\n",
    "            for i in range(len(words) - n_gram):\n",
    "                context_embeddings = embeddings[i:i+n_gram].view(-1)  # Flatten the 5-gram embeddings\n",
    "                inputs.append(context_embeddings)\n",
    "                \n",
    "                target_word = words[i + n_gram]\n",
    "\n",
    "                if target_word in word2idx:\n",
    "                    target_idx = word2idx[target_word]\n",
    "                else:\n",
    "                    target_idx = word2idx['<UNK>']\n",
    "\n",
    "                targets.append(target_idx)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sentence: {sentence}\")\n",
    "            print(e)\n",
    "            continue\n",
    "    \n",
    "    print(f\"Inputs shape:{len(inputs)}\")\n",
    "    print(f\"Targets shape:{len(targets)}\")\n",
    "    inputs = torch.stack(inputs)\n",
    "    targets = torch.tensor(targets)\n",
    "    print(f\"Inputs shape:{inputs.shape}\")\n",
    "    print(f\"Targets shape:{targets.shape}\")\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences: 100%|██████████| 33/33 [00:00<00:00, 56.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape:245\n",
      "Targets shape:245\n",
      "Inputs shape:torch.Size([245, 1500])\n",
      "Targets shape:torch.Size([245])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Exclude the sentences that are shorter than 5 words\n",
    "train_sentences = [sentence for sentence in train_sentences if len(sentence.split()) > 5]\n",
    "\n",
    "train_inputs, train_targets = extract_data(train_sentences, word2idx, embedding_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLanguageModel(nn.Module):\n",
    "    def __init__(self, n_gram, vocab_size, embedding_dim=300, hidden_dim=300, dropout=0.2):\n",
    "        super(NeuralLanguageModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(embedding_dim * n_gram, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)  # Output is vocab size\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        # return self.softmax(x)\n",
    "\n",
    "model = NeuralLanguageModel(n_gram=5, vocab_size=vocab_size, embedding_dim=300, hidden_dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences: 100%|██████████| 4/4 [00:00<00:00, 10.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape:7\n",
      "Targets shape:7\n",
      "Inputs shape:torch.Size([7, 1500])\n",
      "Targets shape:torch.Size([7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_sentences = [sentence for sentence in val_sentences if len(sentence.split()) > 5]\n",
    "val_inputs, val_targets = extract_data(val_sentences, word2idx, embedding_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5) # L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_val_loss(model, val_inputs, val_targets, criterion):\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(range(0, len(val_inputs), batch_size), desc=\"Validation Batches\")\n",
    "        for i in pbar:\n",
    "            inputs = val_inputs[i:i+batch_size].to(device)\n",
    "            targets = val_targets[i:i+batch_size].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "            pbar.set_postfix({\"Current Validation Loss\": loss.item()})\n",
    "    print(f\"Val Loss: {val_loss}\")\n",
    "    print(f\"Number of validation samples: {len(val_inputs)}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    if len(val_inputs) < batch_size:\n",
    "        val_loss /= 1\n",
    "    else:\n",
    "        val_loss /= (len(val_inputs)//batch_size)\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    pbar = tqdm(range(0, len(train_inputs), batch_size), desc=\"Training Batches\")\n",
    "    for i in pbar:\n",
    "        inputs = train_inputs[i:i+batch_size].to(device)\n",
    "        targets = train_targets[i:i+batch_size].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({\"Current Training Loss\": loss.item()})\n",
    "    \n",
    "    train_loss = running_loss / (len(train_inputs)//batch_size)\n",
    "    val_loss = calculate_val_loss(model, val_inputs, val_targets, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'language_model_q1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = torch.load('language_model_q1.pth')\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences: 100%|██████████| 3/3 [00:00<00:00,  6.44it/s]\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [sentence for sentence in test_sentences if len(sentence.split()) > 5]\n",
    "test_inputs, test_targets = extract_data(test_sentences, word2idx, embedding_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Batches: 100%|██████████| 1/1 [00:00<00:00, 59.80it/s, Current Test Loss=6.75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 6.74643087387085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "\n",
    "def calculate_test_loss(model, test_inputs, test_targets, criterion):\n",
    "    test_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(range(0, len(test_inputs), batch_size), desc=\"Test Batches\")\n",
    "        for i in pbar:\n",
    "            inputs = test_inputs[i:i+batch_size].to(device)\n",
    "            targets = test_targets[i:i+batch_size].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "            pbar.set_postfix({\"Current Test Loss\": loss.item()})\n",
    "    \n",
    "    # return test_loss / (len(test_inputs)//batch_size)\n",
    "\n",
    "    if len(test_inputs) < batch_size:\n",
    "        test_loss /= 1\n",
    "    else:\n",
    "        test_loss /= (len(test_inputs)//batch_size)\n",
    "\n",
    "    return test_loss\n",
    "\n",
    "test_loss = calculate_test_loss(model, test_inputs, test_targets, criterion)\n",
    "print(f\"Test Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, sentence, word2idx, embedding_gen, criterion):\n",
    "    model.eval()\n",
    "    words = sentence.split()\n",
    "    embeddings = torch.tensor([embedding_gen.get_embedding(word) for word in words], dtype=torch.float32).to(device)\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(words) - 5):\n",
    "            context_embeddings = embeddings[i:i+5].view(-1)\n",
    "            target_word = words[i+5]\n",
    "            target_idx = word2idx.get(target_word, word2idx['<UNK>'])\n",
    "            target = torch.tensor([target_idx]).to(device)\n",
    "            context_embeddings = context_embeddings.unsqueeze(0)\n",
    "            context_embeddings = context_embeddings.to(device)\n",
    "            output = model(context_embeddings)\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    perplexity = np.exp(total_loss / (len(words) - 5))\n",
    "    return perplexity\n",
    "\n",
    "def compute_perplexity_average(model, sentences, word2idx, embedding_gen, criterion):\n",
    "    perplexities = []\n",
    "    for sentence in sentences:\n",
    "        perplexity = calculate_perplexity(model, sentence, word2idx, embedding_gen, criterion)\n",
    "        perplexities.append(perplexity)\n",
    "    return np.mean(perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences: 100%|██████████| 33/33 [00:00<00:00, 56.89it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('temp_perplexity_sentences_training_q1.txt', 'w') as file:\n",
    "    for sentence in tqdm(train_sentences, desc=\"Processing Sentences\"):\n",
    "        perplexity = calculate_perplexity(model, sentence, word2idx, embedding_gen, criterion)\n",
    "        file.write(f\"{sentence}\\t{perplexity}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences: 100%|██████████| 3/3 [00:00<00:00, 60.96it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('perplexity_sentences_test_q1.txt', 'w') as file:\n",
    "    for sentence in tqdm(test_sentences, desc=\"Processing Sentences\"):\n",
    "        perplexity = calculate_perplexity(model, sentence, word2idx, embedding_gen, criterion)\n",
    "        file.write(f\"{sentence}\\t{perplexity}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_hyperparameters(dropout_rates, hidden_dims, optimizers, criterion, train_X, train_y, val_X, val_y, epochs=10, batch_size=64):\n",
    "    results = []\n",
    "    train_perplexities = []\n",
    "    val_perplexities = []\n",
    "\n",
    "    for dropout in dropout_rates:\n",
    "        for hidden_dim in hidden_dims:\n",
    "            for opt in optimizers:\n",
    "                model = NeuralLanguageModel(n_gram=5, vocab_size=vocab_size, embedding_dim=300, hidden_dim=hidden_dim).to(device)\n",
    "                if opt == optimizers[0]:\n",
    "                    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "                else:\n",
    "                    optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "                for epoch in range(epochs):\n",
    "                    model.train()\n",
    "                    for i in tqdm(range(0, len(train_X), batch_size), desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "                        inputs = train_X[i:i+batch_size].to(device)\n",
    "                        targets = train_y[i:i+batch_size].to(device)\n",
    "                        \n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, targets)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                train_perplexity = compute_perplexity_average(model, train_sentences, word2idx, embedding_gen, criterion)\n",
    "                val_perplexity = compute_perplexity_average(model, val_sentences, word2idx, embedding_gen, criterion)\n",
    "                train_perplexities.append(train_perplexity)\n",
    "                val_perplexities.append(val_perplexity)\n",
    "                print(f\"Train Perplexity: {train_perplexity}, Val Perplexity: {val_perplexity}\")\n",
    "                if opt == optimizers[0]:\n",
    "                    results.append((dropout, hidden_dim, \"Adam\", train_perplexity, val_perplexity))\n",
    "                else:\n",
    "                    results.append((dropout, hidden_dim, \"SGD\", train_perplexity, val_perplexity))\n",
    "    \n",
    "    return results\n",
    "\n",
    "dropout_rates = [0.2, 0.3]\n",
    "hidden_dims = [300, 400]\n",
    "optimizers = [optim.Adam, optim.SGD]\n",
    "\n",
    "results = train_with_hyperparameters(dropout_rates, hidden_dims, optimizers, criterion, train_inputs, train_targets, val_inputs, val_targets, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results, columns=[\"Dropout\", \"Hidden Dim\", \"Optimizer\", \"Train Perplexity\", \"Val Perplexity\"])\n",
    "results_df.to_csv('hyperparameter_results.csv', index=False)\n",
    "\n",
    "print(\"Best Hyperparameters\")\n",
    "print(results_df.loc[results_df['Val Perplexity'].idxmin()])\n",
    "print(\"Worst Hyperparameters\")\n",
    "print(results_df.loc[results_df['Val Perplexity'].idxmax()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
